python -m transformer_chips.playground.benchmark_lora.train \
    --recipe c3 \
    --model_save_path ./model/lora/c3/llama_7b \
    --out_path ./data/benchmark_lora/c3/llama_7b \
    --base_model ./model/Llama-2-7b-hf \
    --max_seq_length 2048 \
    --learning_rate 1e-5 \
    --weight_decay 0.01 \
    --r 16 \
    --lora_alpha 32 \
    --process_batch_size 8 \
    --train_batch_size 1 \
    --epoch 1 \
    --train_example_limit 20000 \
    --logging_steps 100 \
    --seed 42 \
    --fp16